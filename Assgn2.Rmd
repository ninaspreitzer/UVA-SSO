---
title: "Assignment 2 - Group 59"
date: "15. October 2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
highlight: tango
---

```{r, results='hide', echo=FALSE}
sat<- read.table("~/Desktop/SSO/data/sat.txt",sep='',header=TRUE)
tree<- read.table("~/Desktop/SSO/data/treeVolume.txt",sep='',header=TRUE)
```

## Exercise 2.1
**a)**
In order to find the best multiple linear regression model, we performed  a step-up- as well as a step-down-approach, in order to identify which explanatory variables should be taken into account. We got the following models:
Step-up: **total ~ takers + expend** 
Step-down: **total ~ ratio + salary + takers** 

As the approaches result in two different model, we estimate, which model should be preferred, by looking at the following parameters: 

* **p-value:**  Step-up = significant (<0.05) & Step-down = significant (<0.05)
* **Adjusted R^2:**      Step-up = 0.8118 & Step-down = 0.8124
* **Shapiro-Wilk test:** Step-up p.value = 0.5175 & Step-down p-value = 0.772

```{r, fig.height=3, fig.width=6, results='hide'}
#step-up
lm = lm (total ~ 1, data=sat)
step(lm, direction="forward", scope= ~ expend + ratio + salary + takers, data=sat)
lm_up=lm(total ~ takers + expend, data=sat)
#step-down
lm = lm (total ~ expend + ratio + salary + takers, data=sat)
step(lm, direction="backward", scope= ~ expend + ratio + salary + takers, data=sat)
lm_down=lm(total ~ ratio + salary + takers, data=sat)
```

From this examination we can retrieve, that both models are significant, as the p-value is less than 0.05. We also look at the adjusted R^2 as it, other than R^2, also takes complexity of the model into account. The number of the model from step-down-apprach is a little higher, with a difference of 0.0006. The variance inflation factor (VIF) show that we don't have a ollinearity problem, as all the numbers are lower than five. For residuals of both models, normality assumption is not rejected as the Shapiro-Wilk test gives a high p-value. Also plotting residuals within a QQ-plot strengthens this assumption, as it follows a straight line. 

```{r, fig.height=3, fig.width=6, results='hide'}
summary(lm_up) #p_value significant + Ad_R^2: 0.8118 
summary(lm_down) #p_value significant + Ad_R^2: 0.8124

#vif(lm_up) #takers: 1.541324, expend: 1.541324
#vif(lm_down) #ratio: 1.07849, salary:1.66166, expend: 1.74067

shapiro.test(residuals(lm_up))#p-value=0.772
shapiro.test(residuals(lm_down))#p-value=0.5175
par(mfrow=c(1,4)); qqnorm(residuals(lm_up)); plot(residuals(lm_up))
qqnorm(residuals(lm_down)); plot(residuals(lm_down)) #almost straight line and no pattern
```

Taking all of this into account, there was no argument found which specifically holds against one of the models. We conclude with preferring the first model **total = 993.8 - 2.85*takers + 12.29*expend** has it has fewer variables and the adjusted R^2 have comparable values. It is the simpler model and is therefore more robust. 

**b)**
After adding the square of takers as an additional explanatory variable, we once again used the step-up and step-down method, to find the best model. This time, both approaches result in the same model: **total ~ takers + takers2 + expend**. As the variable takers2 is now included in the model, we know that it is significant. 

```{r, fig.height=3, fig.width=6, results='hide'}
sat$takers2=(sat$takers)^2
#step-up
lm = lm (total ~ 1, data=sat)
step(lm, direction="forward", scope= ~ expend + ratio + salary + takers + takers2, data=sat)
lm_up2=lm(total ~ takers + takers2 + expend, data=sat)
#step-down
lm = lm (total ~ expend + ratio + salary + takers + takers2, data=sat)
step(lm, direction="backward", scope= ~ expend + ratio + salary + takers + takers2, data=sat)
lm_down2=lm(total ~ expend + takers + takers2, data=sat)
```

**c)**
We now compare the two best model from *a)* (**total ~ takers + expend**) and *b)*  (**total ~ takers + takers2 + expend**) Looking at the summary, we see that both p-values are significant and that the adjusted R^2 of the model from *b)* is around 6% higher than from *a)*. By conduicting a Shapiro-Wilcon test on both models we receive a high p-value, therefore we can hold on our assumption that residuals are normally distributed. This assumption is also strenghtend by looking at the graphs of the residual. Both are randomly distributed and show an almost straight line in the QQ-plot. 
```{r, fig.height=3, fig.width=6, results='hide'}
best_lm1 = lm(total ~ takers + expend, data = sat)
best_lm2 = lm(total ~ expend + takers + takers2, data = sat)
summary(best_lm1) #p_value significant + Ad_R^2 0.8118
summary(best_lm2) #p_value significant + Ad_R^2 0.8785

shapiro.test(residuals(best_lm1)) #p_value 0.772
shapiro.test(residuals(best_lm2)) #p_value 0.7568
par(mfrow=c(1,4)); qqnorm(residuals(best_lm1)); plot(residuals(best_lm1))
qqnorm(residuals(best_lm2)); plot(residuals(best_lm2)) #almost straight line and no pattern
```

Additionally, we took a look at the VIF. The first model does not show a collinearity problem, as all the numbers are lower than five. The second model's VIF, on the other hand, result in two numbers higher than 5. When looking at the correlation matrix, we see that takers and takers2  are collinear, as they hold a correlation value of 0.98. Keeping in mind that takers2 is simply the squared takers-variable, it makes sense that there is a relation between those explanatory variables. 

```{r, fig.height=3, fig.width=6}
#vif(best_lm1) #takers: 1.541324, expend: 1.541324
#vif(best_lm2) #expend: 1.636819, takers: 25.536969 takers: 26.885327
sat_sub=subset(sat, select=c("expend", "takers", "takers2")) #subset of only relevant x's 
round(cor(sat_sub),2) 
```

As a conclusion, we prefer the simpler model from *a)*, as it does not have a collinearity problem. In the trade-off of higher R^2 and simplicity, we descide to go with simplicity. 

```{r, fig.height=3, fig.width=6, echo=FALSE}
lm(total ~ takers + expend, data = sat)
```
$total = 993.8 - 2.85*takers + 12.29*expend$ is our final model. The intercept $\beta_0$ of is a total SAT score around 994. -2.85 ($= \beta_1$) represents the marginal change in the total score ($=y$) that occurs when takers ($=x_1$) changes by one unit. Also, when expend ($=x_2$) increases by one unit, the marginal change of the total score ($=y$) is +12.29 ($= \beta_2$)

**d)**
We use our preferred model to construct a confidence interval, which gives us an interval for the mean of the new total score for given new x-values (stored in $newxdata$). The resulting confidence interval is $[972.8691, 995.1129]$. Additionally, we looked at prediction interval, which is used for an individual observation of the new total score for givem new x's. Here we receive the following interval $[917.7506, 1050.231]$. As you can see the prection interval is larger than the confidence iterval, as it takes the error of the model also into account. 

```{r, fig.height=3, results='hide'}
newxdata=data.frame(expend=5, ratio=20, salary=36.000, takers=25)
predict(best_lm1,newxdata,interval="confidence",level=0.95) 
# fit: 1983.991 lwr: 972.8691 upr: 995.1129
predict(best_lm1,newxdata,interval="prediction",level=0.95) 
# fit: 1983.991 lwr: 917.7506 upr:1050.231
```


## Exercise 2.1
**a)**
While performing an ANOVA-test we test if a factor effect is present ($=H_1$) by taking $type$ of trees as the explanatory variable and predictor variable $volume$. Our $H_0$ is therefore, that there is no factor effect. As the p-value is above 0.05, we don't have significant evidence to proof that statement. 

The summary of our model, shows an intercept of 30.171, which is the estimated volume of beech (as beech is treated as $\alpha_1 = 0$) and an estimated volume of 25,25 for oak ($intercept + 5.079$). Nevertheless, as we could not reject our Nullhypothesis, we can not conclude significantly, that oaks are more voluminous than beechs. 

```{r, fig.height=3, fig.width=6, results='hide'}
treelm=lm(volume~type,data=tree); anova(treelm) # p-value = 0.1736
summary(treelm) #estimate: intercept (beech)=30.171; oak=5.079 
```

**b)**
Additionally to $type$, we now include $height$ and $width$ as explanatory variables and test again if $type$ influences $volume$ of trees. As we now have a mix of continous an categorical variables, we use ANCOVA. We see that also with this test the p-value is higher than 0.05, that means that we don't have significant evidence to conclude that $type$ effects $volume$. Note, that we now only tested for $type$ (last in order) and cannot conclude anything for the other variables.

```{r, fig.height=3, fig.width=6}
treelm=lm(volume~diameter+height+type, data=tree); anova(treelm)
```

________________________DISCUSS WITH TA_________________________


```{r, fig.height=3, fig.width=6, results='hide'}
h=mean(tree$height)
d=mean(tree$diameter)
databeech=data.frame(diameter=d, height=h, type="beech")
dataoak=data.frame(diameter=d, height=h, type="oak")
predict(treelm,databeech) #beech=33.20049 or 32.58138
predict(treelm,dataoak) #beech=31.89589 or 32.58136 
```

```{r, fig.height=3, fig.width=6, results='hide'}
h=mean(tree$height)
d=mean(tree$diameter)
databeech=data.frame(diameter=d, height=h, type="beech")
dataoak=data.frame(diameter=d, height=h, type="oak")
predict(treelm,databeech) #beech=33.20049
predict(treelm,dataoak) #beech=31.89589
```

```{r, fig.height=3, fig.width=6, results='hide'}
treelm=lm(volume~height+type+diameter, data=tree)
anova(treelm) #p-value of diameter<2.2e-16 is significant

treelm=lm(volume~type:diameter, data=tree)
anova(treelm) #p-value of diameter<2.2e-16 is significant
```

____________________________________________________________________
**c)**
In order to receive a better performing model, we transform our explanatory variables. The general formula used to calculate volume is the folowing: $volume = \pi * radius^2 * height$. As we have all needed variables in our dataset ($radius=diameter/2$), we transform a subset of our x's making use of this forumla and save it in a new column ($transf$).

By testing if $transf$ influences volume with a linear regression, we have see that our p-value is lower than 0.05. Therefore we have significant evidence to say that $transf$ effects $volume$. By looking at the summary of our model, we see that the $R^2 = 0.9748$, which says that almost 98% of volume can be expained with the transformed x-values. 
```{r, fig.height=3, fig.width=6}
#V= pi * r^2 +h 
tree$transf= pi * (tree$diameter^2)/4 * (tree$height)
treelm=lm(volume~transf,data=tree) 
summary(treelm)
```

_______________CONCLUSION MISSING IF BETTER PERFORIN, WE WAIT FOR B)________________ 
